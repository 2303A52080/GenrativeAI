{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsqPu5md57KKrFiI4D8PK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52080/GenrativeAI/blob/main/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (1 ponto) Design a simple ANN architecture with one input and one output layer (no hidden\n",
        "layer). Assume a linear activation function in the output layer.\n",
        "• Write Python code for a backpropagation algorithm with gradient descent optimization to\n",
        "update weights and bias parameters of the ANN model with training data shown in Table\n",
        "1.\n",
        "• Calculate the mean square error with training and testing data shown in Table 2.\n",
        "• Write Python code that reads the input data [x1, x2, and x3] from the user. Predict the\n",
        "output with deployed ANN model\n",
        "Tabela 1: Training Data\n",
        "x1 x2 x3 y\n",
        "0.1 0.2 0.3 0.14\n",
        "0.2 0.3 0.4 0.20\n",
        "0.3 0.4 0.5 0.26\n",
        "0.5 0.6 0.7 0.38\n",
        "0.1 0.3 0.5 0.22\n",
        "0.2 0.4 0.6 0.28\n",
        "0.3 0.5 0.7 0.34\n",
        "0.4 0.6 0.8 0.40\n",
        "0.5 0.7 0.1 0.22\n",
        "Tabela 2: Test Data\n",
        "x1 x2 x3 y\n",
        "0.6 0.7 0.8 0.44\n",
        "0.7 0.8 0.9 0.50"
      ],
      "metadata": {
        "id": "g_Utvrxhy3iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZexva1Jyixt",
        "outputId": "86be1251-a60b-46cc-a907-7ea28467bda9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE: 0.07074393807202685\n",
            "Epoch 1000, MSE: 0.0033939108186680244\n",
            "Epoch 2000, MSE: 0.0008263470597302336\n",
            "Epoch 3000, MSE: 0.00017827990277910163\n",
            "Epoch 4000, MSE: 4.4837647572482004e-05\n",
            "Epoch 5000, MSE: 1.952696614528954e-05\n",
            "Epoch 6000, MSE: 1.4405036424620929e-05\n",
            "Epoch 7000, MSE: 1.294364710982312e-05\n",
            "Epoch 8000, MSE: 1.2178404388043618e-05\n",
            "Epoch 9000, MSE: 1.1581017088159461e-05\n",
            "Final Training MSE: 1.1054740713862296e-05\n",
            "Final Testing MSE: 0.00041083463448165374\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Mean Squared Error\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Training data (Tabela 1)\n",
        "x_train = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.2, 0.3, 0.4],\n",
        "    [0.3, 0.4, 0.5],\n",
        "    [0.5, 0.6, 0.7],\n",
        "    [0.1, 0.3, 0.5],\n",
        "    [0.2, 0.4, 0.6],\n",
        "    [0.3, 0.5, 0.7],\n",
        "    [0.4, 0.6, 0.8],\n",
        "    [0.5, 0.7, 0.1]\n",
        "])\n",
        "\n",
        "y_train = np.array([[0.14], [0.20], [0.26], [0.38], [0.22], [0.28], [0.34], [0.40], [0.22]])\n",
        "\n",
        "# Test data (Tabela 2)\n",
        "x_test = np.array([\n",
        "    [0.6, 0.7, 0.8],\n",
        "    [0.7, 0.8, 0.9]\n",
        "])\n",
        "y_test = np.array([[0.44], [0.50]])\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "n_inputs = x_train.shape[1]\n",
        "n_hidden = 4  # Number of hidden neurons\n",
        "n_outputs = 1  # Number of output neurons\n",
        "\n",
        "W1 = np.random.randn(n_inputs, n_hidden)\n",
        "b1 = np.random.randn(n_hidden)\n",
        "W2 = np.random.randn(n_hidden, n_outputs)\n",
        "b2 = np.random.randn(n_outputs)\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    hidden_layer_input = np.dot(x_train, W1) + b1\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
        "    y_pred = sigmoid(output_layer_input)\n",
        "\n",
        "    # Compute error\n",
        "    error = y_train - y_pred\n",
        "\n",
        "    # Backpropagation\n",
        "    d_output = error * sigmoid_derivative(y_pred)\n",
        "    d_hidden = np.dot(d_output, W2.T) * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 += np.dot(hidden_layer_output.T, d_output) * learning_rate\n",
        "    b2 += np.sum(d_output, axis=0) * learning_rate\n",
        "    W1 += np.dot(x_train.T, d_hidden) * learning_rate\n",
        "    b1 += np.sum(d_hidden, axis=0) * learning_rate\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse(y_train, y_pred)}\")\n",
        "\n",
        "# Evaluate on training and test data\n",
        "y_train_pred = sigmoid(np.dot(sigmoid(np.dot(x_train, W1) + b1), W2) + b2)\n",
        "y_test_pred = sigmoid(np.dot(sigmoid(np.dot(x_test, W1) + b1), W2) + b2)\n",
        "\n",
        "print(\"Final Training MSE:\", mse(y_train, y_train_pred))\n",
        "print(\"Final Testing MSE:\", mse(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_l0tH5QcyovZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Mean Squared Error\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Training data (Tabela 1)\n",
        "x_train = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.2, 0.3, 0.4],\n",
        "    [0.3, 0.4, 0.5],\n",
        "    [0.5, 0.6, 0.7],\n",
        "    [0.1, 0.3, 0.5],\n",
        "    [0.2, 0.4, 0.6],\n",
        "    [0.3, 0.5, 0.7],\n",
        "    [0.4, 0.6, 0.8],\n",
        "    [0.5, 0.7, 0.1]\n",
        "])\n",
        "\n",
        "y_train = np.array([[0.14], [0.20], [0.26], [0.38], [0.22], [0.28], [0.34], [0.40], [0.22]])\n",
        "\n",
        "# Test data (Tabela 2)\n",
        "x_test = np.array([\n",
        "    [0.6, 0.7, 0.8],\n",
        "    [0.7, 0.8, 0.9]\n",
        "])\n",
        "y_test = np.array([[0.44], [0.50]])\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "n_inputs = x_train.shape[1]\n",
        "n_hidden = 4  # Number of hidden neurons\n",
        "n_outputs = 1  # Number of output neurons\n",
        "\n",
        "W1 = np.random.randn(n_inputs, n_hidden)\n",
        "b1 = np.random.randn(n_hidden)\n",
        "W2 = np.random.randn(n_hidden, n_outputs)\n",
        "b2 = np.random.randn(n_outputs)\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    hidden_layer_input = np.dot(x_train, W1) + b1\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
        "    y_pred = sigmoid(output_layer_input)\n",
        "\n",
        "    # Compute error\n",
        "    error = y_train - y_pred\n",
        "\n",
        "    # Backpropagation\n",
        "    d_output = error * sigmoid_derivative(y_pred)\n",
        "    d_hidden = np.dot(d_output, W2.T) * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 += np.dot(hidden_layer_output.T, d_output) * learning_rate\n",
        "    b2 += np.sum(d_output, axis=0) * learning_rate\n",
        "    W1 += np.dot(x_train.T, d_hidden) * learning_rate\n",
        "    b1 += np.sum(d_hidden, axis=0) * learning_rate\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse(y_train, y_pred)}\")\n",
        "\n",
        "# Evaluate on training and test data\n",
        "y_train_pred = sigmoid(np.dot(sigmoid(np.dot(x_train, W1) + b1), W2) + b2)\n",
        "y_test_pred = sigmoid(np.dot(sigmoid(np.dot(x_test, W1) + b1), W2) + b2)\n",
        "\n",
        "print(\"Final Training MSE:\", mse(y_train, y_train_pred))\n",
        "print(\"Final Testing MSE:\", mse(y_test, y_test_pred))\n",
        "\n",
        "# User input prediction\n",
        "x_input = np.array([list(map(float, input(\"Enter values for x1, x2, x3 separated by space: \").split()))])\n",
        "y_input_pred = sigmoid(np.dot(sigmoid(np.dot(x_input, W1) + b1), W2) + b2)\n",
        "print(\"Predicted Output:\", y_input_pred[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYhcecwxyo42",
        "outputId": "e5fd31b5-8e8e-4e5f-97f8-9cf64bc3bf91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE: 0.07074393807202685\n",
            "Epoch 1000, MSE: 0.0033939108186680244\n",
            "Epoch 2000, MSE: 0.0008263470597302336\n",
            "Epoch 3000, MSE: 0.00017827990277910163\n",
            "Epoch 4000, MSE: 4.4837647572482004e-05\n",
            "Epoch 5000, MSE: 1.952696614528954e-05\n",
            "Epoch 6000, MSE: 1.4405036424620929e-05\n",
            "Epoch 7000, MSE: 1.294364710982312e-05\n",
            "Epoch 8000, MSE: 1.2178404388043618e-05\n",
            "Epoch 9000, MSE: 1.1581017088159461e-05\n",
            "Final Training MSE: 1.1054740713862296e-05\n",
            "Final Testing MSE: 0.00041083463448165374\n",
            "Enter values for x1, x2, x3 separated by space: 0.1 0.2 0.3\n",
            "Predicted Output: 0.14796391632287542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. (1 ponto) Design a simple ANN architecture with one input and one output layer (no hidden\n",
        "layer). Assume a sigmoid activation function shown in the equation 1 in the output layer.\n",
        "f(x) =\n",
        "1\n",
        "1 + e−x (1)\n",
        "• Write Python code for a backpropagation algorithm with gradient descent optimization to\n",
        "update weights and bias parameters of the ANN model with training data shown in Table\n",
        "Tabela 3: Training Data\n",
        "x1 x2 x3 y\n",
        "0.1 0.2 0.3 0.5349\n",
        "0.2 0.3 0.4 0.5498\n",
        "0.3 0.4 0.5 0.5646\n",
        "0.5 0.6 0.7 0.5939\n",
        "0.1 0.3 0.5 0.5548\n",
        "0.2 0.4 0.6 0.5695\n",
        "0.3 0.5 0.7 0.5842\n",
        "0.4 0.6 0.8 0.5987\n",
        "0.5 0.7 0.1 0.5548\n",
        "Tabela 4: Test Data\n",
        "x1 x2 x3 y\n",
        "0.6 0.7 0.8 0.6083\n",
        "0.7 0.8 0.9 0.6225\n",
        "• Calculate the mean square error with training and testing data shown in Table 4.\n",
        "• Write Python code that reads the input data [x1, x2, and x3] from the user. Predict the\n",
        "output with deployed ANN model"
      ],
      "metadata": {
        "id": "1szuYMznyrLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Mean Squared Error\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Training data (Tabela 3)\n",
        "x_train = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.2, 0.3, 0.4],\n",
        "    [0.3, 0.4, 0.5],\n",
        "    [0.5, 0.6, 0.7],\n",
        "    [0.1, 0.3, 0.5],\n",
        "    [0.2, 0.4, 0.6],\n",
        "    [0.3, 0.5, 0.7],\n",
        "    [0.4, 0.6, 0.8],\n",
        "    [0.5, 0.7, 0.1]\n",
        "])\n",
        "\n",
        "y_train = np.array([[0.5349], [0.5498], [0.5646], [0.5939], [0.5548], [0.5695], [0.5842], [0.5987], [0.5548]])\n",
        "\n",
        "# Test data (Tabela 4)\n",
        "x_test = np.array([\n",
        "    [0.6, 0.7, 0.8],\n",
        "    [0.7, 0.8, 0.9]\n",
        "])\n",
        "y_test = np.array([[0.6083], [0.6225]])\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "n_inputs = x_train.shape[1]\n",
        "n_outputs = 1  # Single output neuron\n",
        "\n",
        "W = np.random.randn(n_inputs, n_outputs)\n",
        "b = np.random.randn(n_outputs)\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    output_layer_input = np.dot(x_train, W) + b\n",
        "    y_pred = sigmoid(output_layer_input)\n",
        "\n",
        "    # Compute error\n",
        "    error = y_train - y_pred\n",
        "\n",
        "    # Backpropagation\n",
        "    d_output = error * sigmoid_derivative(y_pred)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W += np.dot(x_train.T, d_output) * learning_rate\n",
        "    b += np.sum(d_output, axis=0) * learning_rate\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse(y_train, y_pred)}\")\n",
        "\n",
        "# Evaluate on training and test data\n",
        "y_train_pred = sigmoid(np.dot(x_train, W) + b)\n",
        "y_test_pred = sigmoid(np.dot(x_test, W) + b)\n",
        "\n",
        "print(\"Final Training MSE:\", mse(y_train, y_train_pred))\n",
        "print(\"Final Testing MSE:\", mse(y_test, y_test_pred))\n",
        "\n",
        "# User input prediction\n",
        "x_input = np.array([list(map(float, input(\"Enter values for x1, x2, x3 separated by space: \").split()))])\n",
        "y_input_pred = sigmoid(np.dot(x_input, W) + b)\n",
        "print(\"Predicted Output:\", y_input_pred[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4D7Es2Vyrap",
        "outputId": "65428897-234d-4f9c-84d7-c0fb8e5e7b3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE: 0.09353999577880288\n",
            "Epoch 1000, MSE: 5.544934596807592e-05\n",
            "Epoch 2000, MSE: 2.3131250900065538e-05\n",
            "Epoch 3000, MSE: 1.971958459615244e-05\n",
            "Epoch 4000, MSE: 1.7440837893454712e-05\n",
            "Epoch 5000, MSE: 1.5444969673883977e-05\n",
            "Epoch 6000, MSE: 1.367788366527421e-05\n",
            "Epoch 7000, MSE: 1.2112805178661574e-05\n",
            "Epoch 8000, MSE: 1.0726656840344804e-05\n",
            "Epoch 9000, MSE: 9.499009334937893e-06\n",
            "Final Training MSE: 8.411761505499611e-06\n",
            "Final Testing MSE: 2.933716127934951e-05\n",
            "Enter values for x1, x2, x3 separated by space: 1 2 3\n",
            "Predicted Output: 0.7741848554882129\n"
          ]
        }
      ]
    }
  ]
}