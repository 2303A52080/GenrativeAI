{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVYO4bScQUrXye0WUHOxqu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52080/GenrativeAI/blob/main/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI - Assignment - 5.2\n",
        "Instructions:\n",
        "1. (1 ponto) Design a multi-layer ANN architecture with one input, one hidden, and one output\n",
        "layer. Assume a linear activation function in the output layer and a sigmoid activation function\n",
        "in the hidden layer.\n",
        "• Write Python code for a backpropagation algorithm with gradient descent optimization to\n",
        "update weights and bias parameters of the ANN model with training data shown in Table\n",
        "1.\n",
        "• Calculate the mean square error with training and testing data shown in Table 2.\n",
        "• Write Python code that reads the input data [x1 and x2] from the user. Predict the output\n",
        "with deployed ANN model\n",
        "Tabela 1: Training Data\n",
        "x1 x2 y\n",
        "0.2 0.1 0.3441\n",
        "0.3 0.2 0.3500\n",
        "0.4 0.3 0.3558\n",
        "0.7 0.6 0.3729\n",
        "0.8 0.7 0.3785\n",
        "0.9 0.8 0.3841\n",
        "Tabela 2: Test Data\n",
        "x1 x2 y\n",
        "0.5 0.4 0.3615\n",
        "0.6 0.5 0.3672\n",
        "• Expected learning Outcomes from this assignment related to python\n",
        "– Students are able to understand how backpropagation algorithm helps to update model\n",
        "parameters of multilayer ANN\n",
        "– Students are able to write code in python for backpropagation algorithm\n",
        "– Students are able to design architecture of ANN based on problem statement\n",
        "– Students are able to derive mathematical expression for change in weights and bias\n",
        "parameters for different activation functions\n",
        "• Naming cinvention\n",
        "– Report File Name: RollNo_Week No._Assignment No."
      ],
      "metadata": {
        "id": "wfllMmSmW51K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk4z1zBlVw1Y",
        "outputId": "77684a42-c97a-4be1-9d7a-8d036183eff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.2593\n",
            "Epoch 1000, Loss: 0.0000\n",
            "Epoch 2000, Loss: 0.0000\n",
            "Epoch 3000, Loss: 0.0000\n",
            "Epoch 4000, Loss: 0.0000\n",
            "Epoch 5000, Loss: 0.0000\n",
            "Epoch 6000, Loss: 0.0000\n",
            "Epoch 7000, Loss: 0.0000\n",
            "Epoch 8000, Loss: 0.0000\n",
            "Epoch 9000, Loss: 0.0000\n",
            "Training MSE: 0.0000\n",
            "Test MSE: 0.0000\n",
            "Enter x1: 0.5\n",
            "Enter x2: 0.4\n",
            "Predicted Output for [0.5, 0.4]: 0.3624\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Define Activation Functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Step 2: Prepare Training and Test Data\n",
        "X_train = np.array([[0.2, 0.1], [0.3, 0.2], [0.4, 0.3], [0.7, 0.6], [0.8, 0.7], [0.9, 0.8]])\n",
        "y_train = np.array([[0.3441], [0.3500], [0.3558], [0.3729], [0.3785], [0.3841]])\n",
        "X_test = np.array([[0.5, 0.4], [0.6, 0.5]])\n",
        "y_test = np.array([[0.3615], [0.3672]])\n",
        "\n",
        "# Step 3: Define ANN Architecture\n",
        "input_layer_neurons = 2\n",
        "hidden_layer_neurons = 3\n",
        "output_layer_neurons = 1\n",
        "\n",
        "# Step 4: Initialize Weights and Biases randomly\n",
        "weights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
        "bias_hidden = np.random.uniform(size=(1, hidden_layer_neurons))\n",
        "weights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons))\n",
        "bias_output = np.random.uniform(size=(1, output_layer_neurons))\n",
        "\n",
        "# Learning parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Step 5: Train the Model\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    hidden_layer_activation = np.dot(X_train, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = output_layer_activation  # Linear activation for output\n",
        "\n",
        "    # Compute the error\n",
        "    error = y_train - predicted_output\n",
        "\n",
        "    # Backpropagation\n",
        "    d_predicted_output = error\n",
        "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "    weights_input_hidden += X_train.T.dot(d_hidden_layer) * learning_rate\n",
        "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    # Print the loss at every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(y_train - predicted_output))\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "# Step 6: Calculate MSE for training and testing datasets\n",
        "mse_train = np.mean(np.square(y_train - predicted_output))\n",
        "print(f'Training MSE: {mse_train:.4f}')\n",
        "\n",
        "hidden_layer_activation_test = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
        "hidden_layer_output_test = sigmoid(hidden_layer_activation_test)\n",
        "\n",
        "output_layer_activation_test = np.dot(hidden_layer_output_test, weights_hidden_output) + bias_output\n",
        "predicted_output_test = output_layer_activation_test  # Linear activation for output\n",
        "\n",
        "mse_test = np.mean(np.square(y_test - predicted_output_test))\n",
        "print(f'Test MSE: {mse_test:.4f}')\n",
        "\n",
        "# Step 7: Make Predictions for new user-input values\n",
        "def predict(x1, x2):\n",
        "    input_data = np.array([[x1, x2]])\n",
        "    hidden_layer_activation = np.dot(input_data, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = output_layer_activation  # Linear activation for output\n",
        "\n",
        "    return predicted_output\n",
        "\n",
        "# Read input data from user\n",
        "x1 = float(input(\"Enter x1: \"))\n",
        "x2 = float(input(\"Enter x2: \"))\n",
        "predicted_output_user = predict(x1, x2)\n",
        "print(f'Predicted Output for [{x1}, {x2}]: {predicted_output_user[0][0]:.4f}')\n"
      ]
    }
  ]
}